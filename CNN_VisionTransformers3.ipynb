{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Deep Learning (AI5100): Assignment-3\n",
        "\n",
        "Name - ARIF KHAN PATHAN (CS23MTECH11024)"
      ],
      "metadata": {
        "id": "NlRKIMy1iWXe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Self-Attention for Object Recognition with CNNs"
      ],
      "metadata": {
        "id": "sn8pkzOoiNG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing required packages\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "VQ3chM8PiK-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# self-attention mechanism\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        # Query convolutional layer\n",
        "        self.query_conv = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n",
        "\n",
        "        # Key convolutional layer\n",
        "        self.key_conv = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n",
        "\n",
        "        # Value convolutional layer\n",
        "        self.value_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
        "\n",
        "        # Scaling factor gamma -> zero\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    # Forward function\n",
        "    def forward(self, x):\n",
        "        # Compute query, key, and value\n",
        "        Q = self.query_conv(x)\n",
        "        Q = Q.view(Q.size(0), -1, Q.size(2) * Q.size(3))\n",
        "        Q = Q.permute(0, 2, 1)\n",
        "\n",
        "        K = self.key_conv(x)\n",
        "        K = K.view(K.size(0), -1, K.size(2) * K.size(3))\n",
        "\n",
        "        energy = torch.bmm(Q, K)\n",
        "        attention = F.softmax(energy, dim=-1)\n",
        "\n",
        "        V = self.value_conv(x)\n",
        "        V = V.view(V.size(0), -1, V.size(2) * V.size(3))\n",
        "\n",
        "        # Reshaping and apply scaling\n",
        "        output = torch.bmm(V, attention.permute(0, 2, 1))\n",
        "        output = self.gamma * output.view(x.size()) + x\n",
        "        # returning output\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "oUMBhyq6ijCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the CNN architecture with self-attention\n",
        "class CNNWithSelfAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNWithSelfAttention, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "        self.sa1 = SelfAttention(16)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.sa2 = SelfAttention(32)\n",
        "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.sa3 = SelfAttention(64)\n",
        "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.sa4 = SelfAttention(128)\n",
        "        self.conv5 = nn.Conv2d(128, 10, kernel_size=3, padding=1)\n",
        "\n",
        "        # Global average pooling\n",
        "        # Reduce the spatial dimensions of image maps while preserving important information.\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.sa1(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.sa2(x)\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = self.sa3(x)\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = self.sa4(x)\n",
        "        x = self.conv5(x)\n",
        "\n",
        "        # Pooling function for Image\n",
        "        x = self.global_avg_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return F.log_softmax(x, dim=1)\n"
      ],
      "metadata": {
        "id": "MTVnSxDsijzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CIFAR-10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n"
      ],
      "metadata": {
        "id": "Z-3qiee6iqPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model, optimizer, and loss function\n",
        "model = CNNWithSelfAttention()\n",
        "\n",
        "# Defining Optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "# Cross Entropy loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(data)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}')\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), 'cnn_with_self_attention.pth')\n"
      ],
      "metadata": {
        "id": "xCItjxCJislo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# Disable gradient calculation for evaluation\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "# Calculate the accuracy of the network on the entire test dataset\n",
        "accuracy_CNN = 100 * correct / total\n",
        "\n",
        "# Print the accuracy\n",
        "print('Accuracy : %d %%' % accuracy_CNN)\n"
      ],
      "metadata": {
        "id": "9M7obkoFivHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) Object Recognition with Vision Transformer"
      ],
      "metadata": {
        "id": "UGNWKEtCiwYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing required packages\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import bmm\n",
        "from torch.optim import Adam\n",
        "from torch.nn import Parameter\n",
        "\n",
        "num_channels = 3"
      ],
      "metadata": {
        "id": "SaqHlPesi1WX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multihead Attention mechanism\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        self.query_conv = nn.Linear(embed_dim, embed_dim)\n",
        "        self.key_conv = nn.Linear(embed_dim, embed_dim)\n",
        "        self.value_conv = nn.Linear(embed_dim, embed_dim)\n",
        "        self.output = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    # Forward pass for the multihead attention\n",
        "    def forward(self, query, key, value):\n",
        "        # Input x shape: (batch_size, seq_len, embed_dim)\n",
        "        batch_size = query.shape[0]\n",
        "\n",
        "        # Split the embedding into multiple heads and perform linear transformations\n",
        "        Q = self.query_conv(query).view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)  # (batch_size, num_heads, seq_len, head_dim)\n",
        "        K = self.key_conv(key).view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        V = self.value_conv(value).view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "\n",
        "        # Calculate attention scores\n",
        "        qk = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.head_dim**0.5\n",
        "\n",
        "        # Apply softmax to get attention weights\n",
        "        attention = torch.softmax(qk, dim=-1)\n",
        "\n",
        "        # Apply attention to value\n",
        "        output = torch.matmul(attention, V)\n",
        "\n",
        "        # Reshape and concatenate attention heads\n",
        "        output = output.permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.embed_dim)\n",
        "\n",
        "        # Apply output linear layer\n",
        "        output = self.output(output)\n",
        "\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "ig8hFx3li5Sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Vision Transformer (ViT) Encoder block with Custom Multi-Head Attention\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout=0.1):\n",
        "        super(EncoderBlock, self).__init__()\n",
        "        self.attn = MultiHeadAttention(embed_dim, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, mlp_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(mlp_dim, embed_dim)\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Multi-head self-attention\n",
        "        attention_output = self.attn(x, x, x)\n",
        "        x = self.norm1(x + self.dropout(attention_output))\n",
        "\n",
        "        # Feedforward network\n",
        "        mlp_output = self.mlp(x)\n",
        "        x = self.norm2(x + self.dropout(mlp_output))\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "2WFXlqtzjDkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Class for Vision transformer\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, image_size=224, patch_size=16, num_classes=10, embed_dim = 768, num_heads = 8, num_layers = 6, mlp_dim = 3072, dropout=0.1):\n",
        "        super(VisionTransformer, self).__init__()\n",
        "        assert image_size % patch_size == 0, \"Image size must be divisible by patch size.\"\n",
        "        num_patches = (image_size // patch_size) ** 2\n",
        "        patch_dim = 3 * patch_size ** 2  # RGB channels * patch_size * patch_size\n",
        "\n",
        "        # Patch embedding layer\n",
        "        self.patch_embedding = nn.Conv2d(num_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "        self.cls_token = Parameter(torch.randn(1, 1, embed_dim))  # Class token\n",
        "\n",
        "        # Positional embeddings\n",
        "        self.positional_embedding = Parameter(torch.randn(1, (image_size // patch_size) ** 2 + 1, embed_dim))\n",
        "\n",
        "        # Encoder blocks\n",
        "        self.encoder = nn.ModuleList([\n",
        "            EncoderBlock(embed_dim, num_heads, mlp_dim, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "\n",
        "        # Classification head\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.fc = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        A= x.shape[0]\n",
        "        x = self.patch_embedding(x).flatten(2).transpose(1, 2)  # Output: (batch_size, embed_dim, grid_size, grid_size)\n",
        "        cls_tokens = self.cls_token.expand(A, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)#add cls tokens\n",
        "        x = x + self.positional_embedding  # Add positional embeddings\n",
        "\n",
        "        # Transformer encoder blocks\n",
        "        for encoder_block in self.encoder:\n",
        "            x = encoder_block(x)\n",
        "\n",
        "         # Take only the first token (class token) output and apply classification layer\n",
        "        x = self.norm(x[:, 0])\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "R2CTrAlnjIif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CIFAR-10 dataset loading and preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(224),  # Resize images to 32x32 (CIFAR-10 size)\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n"
      ],
      "metadata": {
        "id": "UgzJ_JeEjK_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the model on the dataset\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "5TiV5M-WjO4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train the Vision Transformer model with Custom Multi-Head Attention\n",
        "model2 = VisionTransformer().to(device)\n",
        "optimizer = torch.optim.Adam(model2.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model2.train()\n",
        "    running_loss = 0.0\n",
        "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model2(data)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}')\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model2.state_dict(), 'vision_transformer.pth')\n"
      ],
      "metadata": {
        "id": "hErhwOgujP9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the model to evaluation mode\n",
        "model2.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "# Disable gradient calculation for evaluation\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        outputs = model2(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "# Calculate the accuracy of the network on the entire test dataset\n",
        "accuracy_VIT = 100 * correct / total\n",
        "\n",
        "# Print the accuracy\n",
        "print('Accuracy : %d %%' % accuracy_VIT)"
      ],
      "metadata": {
        "id": "Kul2UhZzjY-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Accuracy of CNN with Self-Attention on test images: %.2f %%' % accuracy_CNN)\n",
        "print('Accuracy of the Vision Transformer on test images: %.2f %%' % accuracy_VIT)"
      ],
      "metadata": {
        "id": "p85VctB0jeX6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}